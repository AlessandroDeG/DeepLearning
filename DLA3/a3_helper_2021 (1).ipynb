{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "a3_helper_2021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JILVek0PhzbT"
      },
      "source": [
        "# Helper Code for Assignment 3 (RNN language models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGxdOkx_htjZ"
      },
      "source": [
        "## Reading raw text file & Create DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EawFomcOh3XE"
      },
      "source": [
        "import os\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A5DmY30hHHP"
      },
      "source": [
        "class Vocabulary:\n",
        "\n",
        "    def __init__(self, pad_token=\"<pad>\", unk_token='<unk>'):\n",
        "        self.id_to_string = {}\n",
        "        self.string_to_id = {}\n",
        "        \n",
        "        # add the default pad token\n",
        "        self.id_to_string[0] = pad_token\n",
        "        self.string_to_id[pad_token] = 0\n",
        "        \n",
        "        # add the default unknown token\n",
        "        self.id_to_string[1] = unk_token\n",
        "        self.string_to_id[unk_token] = 1        \n",
        "        \n",
        "        # shortcut access\n",
        "        self.pad_id = 0\n",
        "        self.unk_id = 1\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.id_to_string)\n",
        "\n",
        "    def add_new_word(self, string):\n",
        "        self.string_to_id[string] = len(self.string_to_id)\n",
        "        self.id_to_string[len(self.id_to_string)] = string\n",
        "\n",
        "    # Given a string, return ID\n",
        "    def get_idx(self, string, extend_vocab=False):\n",
        "        if string in self.string_to_id:\n",
        "            return self.string_to_id[string]\n",
        "        elif extend_vocab:  # add the new word\n",
        "            self.add_new_word(string)\n",
        "            return self.string_to_id[string]\n",
        "        else:\n",
        "            return self.unk_id\n",
        "\n",
        "\n",
        "# Read the raw txt file and generate a 1D PyTorch tensor\n",
        "# containing the whole text mapped to sequence of token IDs, and a vocab object.\n",
        "class LongTextData:\n",
        "\n",
        "    def __init__(self, file_path, vocab=None, extend_vocab=True, device='cuda'):\n",
        "        self.data, self.vocab = self.text_to_data(file_path, vocab, extend_vocab, device)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def text_to_data(self, text_file, vocab, extend_vocab, device):\n",
        "        \"\"\"Read a raw text file and create its tensor and the vocab.\n",
        "\n",
        "        Args:\n",
        "          text_file: a path to a raw text file.\n",
        "          vocab: a Vocab object\n",
        "          extend_vocab: bool, if True extend the vocab\n",
        "          device: device\n",
        "\n",
        "        Returns:\n",
        "          Tensor representing the input text, vocab file\n",
        "\n",
        "        \"\"\"\n",
        "        assert os.path.exists(text_file)\n",
        "        if vocab is None:\n",
        "            vocab = Vocabulary()\n",
        "\n",
        "        data_list = []\n",
        "\n",
        "        # Construct data\n",
        "        full_text = []\n",
        "        print(f\"Reading text file from: {text_file}\")\n",
        "        with open(text_file, 'r') as text:\n",
        "            for line in text:\n",
        "                tokens = list(line)\n",
        "                for token in tokens:\n",
        "                    # get index will extend the vocab if the input\n",
        "                    # token is not yet part of the text.\n",
        "                    full_text.append(vocab.get_idx(token, extend_vocab=extend_vocab))\n",
        "\n",
        "        # convert to tensor\n",
        "        data = torch.tensor(full_text, device=device, dtype=torch.int64)\n",
        "        print(\"Done.\")\n",
        "\n",
        "        return data, vocab\n",
        "    \n",
        "\n",
        "# Since there is no need for schuffling the data, we just have to split\n",
        "# the text data according to the batch size and bptt length.\n",
        "# The input to be fed to the model will be batch[:-1]\n",
        "# The target to be used for the loss will be batch[1:]\n",
        "class ChunkedTextData:\n",
        "\n",
        "    def __init__(self, data, bsz, bptt_len, pad_id):\n",
        "        self.batches = self.create_batch(data, bsz, bptt_len, pad_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.batches[idx]\n",
        "\n",
        "    def create_batch(self, input_data, bsz, bptt_len, pad_id):\n",
        "        \"\"\"Create batches from a TextData object .\n",
        "\n",
        "        Args:\n",
        "          input_data: a TextData object.\n",
        "          bsz: int, batch size\n",
        "          bptt_len: int, bptt length\n",
        "          pad_id: int, ID of the padding token\n",
        "\n",
        "        Returns:\n",
        "          List of tensors representing batches\n",
        "\n",
        "        \"\"\"\n",
        "        batches = []  # each element in `batches` is (len, B) tensor\n",
        "        text_len = len(input_data)\n",
        "        segment_len = text_len // bsz + 1\n",
        "\n",
        "        # Question: Explain the next two lines!\n",
        "        padded = input_data.data.new_full((segment_len * bsz,), pad_id)\n",
        "        padded[:text_len] = input_data.data\n",
        "        padded = padded.view(bsz, segment_len).t()\n",
        "        num_batches = segment_len // bptt_len + 1\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            # Prepare batches such that the last symbol of the current batch\n",
        "            # is the first symbol of the next batch.\n",
        "            if i == 0:\n",
        "                # Append a dummy start symbol using pad token\n",
        "                batch = torch.cat(\n",
        "                    [padded.new_full((1, bsz), pad_id),\n",
        "                     padded[i * bptt_len:(i + 1) * bptt_len]], dim=0)\n",
        "                batches.append(batch)\n",
        "            else:\n",
        "                batches.append(padded[i * bptt_len - 1:(i + 1) * bptt_len])\n",
        "\n",
        "        return batches"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u-9Q7K4hVbL",
        "outputId": "8f534e4e-57ae-4534-e6f7-fbcc4e2be7c1"
      },
      "source": [
        "# downlaod the text\n",
        "# Make sure to go to the link and check how the text looks like.\n",
        "\n",
        "!wget http://www.gutenberg.org/files/49010/49010-0.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-16 16:32:52--  http://www.gutenberg.org/files/49010/49010-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/files/49010/49010-0.txt [following]\n",
            "--2021-11-16 16:32:52--  https://www.gutenberg.org/files/49010/49010-0.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 185303 (181K) [text/plain]\n",
            "Saving to: ‘49010-0.txt’\n",
            "\n",
            "49010-0.txt         100%[===================>] 180.96K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-11-16 16:33:00 (1.76 MB/s) - ‘49010-0.txt’ saved [185303/185303]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRqPHT_zhgNa"
      },
      "source": [
        "# This is for Colab. Adapt the path if needed.\n",
        "\n",
        "text_path = \"/content/49010-0.txt\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hff1FxeohHHR",
        "outputId": "664a5aaf-a9b7-4e74-cebe-92e5e5d3f9ca"
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "\n",
        "batch_size = 32\n",
        "bptt_len = 64\n",
        "\n",
        "my_data = LongTextData(text_path, device=DEVICE)\n",
        "batches = ChunkedTextData(my_data, batch_size, bptt_len, pad_id=0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading text file from: /content/49010-0.txt\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTX4uKEChHHT",
        "outputId": "c44ab419-21bf-4b6d-8083-4e24995c2e1d"
      },
      "source": [
        "len(batches)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhGXZebcmShK",
        "outputId": "8a22991c-098f-49b7-c61b-378d3b2cc463"
      },
      "source": [
        "batches[0].shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([65, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kteefy8hmZZz",
        "outputId": "96e9390f-ee2a-45e1-e642-0c5839c6bd33"
      },
      "source": [
        "# input to the network\n",
        "print(batches[0][:-1].shape)\n",
        "batches[0][:-1, 0]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 32])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  2,  3,  4,  5,  6,  7,  8,  9, 10,  5, 11, 12,  6, 13, 14, 12,  5,\n",
              "        15, 16,  5,  8, 17,  6, 18, 19,  9,  9, 20,  6,  9, 21,  6, 22,  5, 23,\n",
              "         9, 24, 25, 23,  6, 26, 27, 16, 28,  5, 23, 29,  6, 16, 30,  6, 31, 32,\n",
              "         6, 33, 32,  6, 34, 12, 35, 11, 20, 15], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIOuzL7kms2Y",
        "outputId": "6089d3a1-2fd3-412c-965d-74c6768af41b"
      },
      "source": [
        "# target tokens to be predicted\n",
        "print(batches[0][1:].shape)\n",
        "batches[0][1:, 0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 32])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10,  5, 11, 12,  6, 13, 14, 12,  5, 15,\n",
              "        16,  5,  8, 17,  6, 18, 19,  9,  9, 20,  6,  9, 21,  6, 22,  5, 23,  9,\n",
              "        24, 25, 23,  6, 26, 27, 16, 28,  5, 23, 29,  6, 16, 30,  6, 31, 32,  6,\n",
              "        33, 32,  6, 34, 12, 35, 11, 20, 15,  5], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CySMFzuXhHHT",
        "outputId": "a13afed0-5c42-44a3-ef60-208563ba7de1"
      },
      "source": [
        "# last token of the current batch should be the first token of the next one:\n",
        "for i in batches[20][:, 11]:\n",
        "    print(my_data.vocab.id_to_string[i.item()])\n",
        "print('================')\n",
        "for i in batches[21][:, 11]:\n",
        "    print(my_data.vocab.id_to_string[i.item()])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "d\n",
            "i\n",
            "n\n",
            "n\n",
            "e\n",
            "r\n",
            " \n",
            "f\n",
            "o\n",
            "r\n",
            " \n",
            "w\n",
            "h\n",
            "i\n",
            "c\n",
            "h\n",
            " \n",
            "n\n",
            "o\n",
            "t\n",
            "h\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "w\n",
            "a\n",
            "s\n",
            " \n",
            "p\n",
            "r\n",
            "o\n",
            "v\n",
            "i\n",
            "d\n",
            "e\n",
            "d\n",
            " \n",
            "b\n",
            "u\n",
            "t\n",
            " \n",
            "a\n",
            "\n",
            "\n",
            "s\n",
            "o\n",
            "u\n",
            "p\n",
            ",\n",
            " \n",
            "w\n",
            "h\n",
            "i\n",
            "c\n",
            "h\n",
            " \n",
            "w\n",
            "a\n",
            "s\n",
            " \n",
            "s\n",
            "e\n",
            "r\n",
            "v\n",
            "================\n",
            "v\n",
            "e\n",
            "d\n",
            " \n",
            "o\n",
            "n\n",
            " \n",
            "a\n",
            " \n",
            "w\n",
            "i\n",
            "d\n",
            "e\n",
            ",\n",
            " \n",
            "s\n",
            "h\n",
            "a\n",
            "l\n",
            "l\n",
            "o\n",
            "w\n",
            " \n",
            "d\n",
            "i\n",
            "s\n",
            "h\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "T\n",
            "h\n",
            "e\n",
            " \n",
            "F\n",
            "o\n",
            "x\n",
            " \n",
            "p\n",
            "r\n",
            "e\n",
            "s\n",
            "i\n",
            "d\n",
            "e\n",
            "d\n",
            " \n",
            "a\n",
            "t\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "f\n",
            "e\n",
            "a\n",
            "s\n",
            "t\n",
            " \n",
            "w\n",
            "i\n",
            "t\n",
            "h\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKA9GA3ihHHU",
        "outputId": "cf10c100-f624-4afb-d29a-20a6e1bd20be"
      },
      "source": [
        "print(my_data.vocab.id_to_string)\n",
        "print(my_data.vocab.string_to_id)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '<pad>', 1: '<unk>', 2: '\\ufeff', 3: 'T', 4: 'h', 5: 'e', 6: ' ', 7: 'P', 8: 'r', 9: 'o', 10: 'j', 11: 'c', 12: 't', 13: 'G', 14: 'u', 15: 'n', 16: 'b', 17: 'g', 18: 'E', 19: 'B', 20: 'k', 21: 'f', 22: 'A', 23: 's', 24: 'p', 25: \"'\", 26: 'F', 27: 'a', 28: 'l', 29: ',', 30: 'y', 31: 'J', 32: '.', 33: 'H', 34: 'S', 35: 'i', 36: '\\n', 37: 'w', 38: 'U', 39: 'd', 40: 'm', 41: 'v', 42: 'Y', 43: '-', 44: 'L', 45: 'I', 46: ':', 47: 'V', 48: 'R', 49: 'C', 50: 'D', 51: 'M', 52: '2', 53: '1', 54: '0', 55: '5', 56: '[', 57: '#', 58: '4', 59: '9', 60: ']', 61: '8', 62: '*', 63: 'O', 64: 'N', 65: 'K', 66: '/', 67: 'W', 68: 'X', 69: '(', 70: '3', 71: ')', 72: 'Æ', 73: '’', 74: '_', 75: '—', 76: 'æ', 77: '·', 78: 'z', 79: 'x', 80: 'q', 81: ';', 82: '6', 83: '7', 84: 'Q', 85: 'œ', 86: 'μ', 87: 'α', 88: 'λ', 89: 'ο', 90: 'ν', 91: 'ὁ', 92: 'Φ', 93: 'ρ', 94: 'ὑ', 95: 'ξ', 96: '“', 97: '”', 98: '?', 99: '!', 100: '‘', 101: 'é', 102: 'è', 103: '\"', 104: '%', 105: '@', 106: '$'}\n",
            "{'<pad>': 0, '<unk>': 1, '\\ufeff': 2, 'T': 3, 'h': 4, 'e': 5, ' ': 6, 'P': 7, 'r': 8, 'o': 9, 'j': 10, 'c': 11, 't': 12, 'G': 13, 'u': 14, 'n': 15, 'b': 16, 'g': 17, 'E': 18, 'B': 19, 'k': 20, 'f': 21, 'A': 22, 's': 23, 'p': 24, \"'\": 25, 'F': 26, 'a': 27, 'l': 28, ',': 29, 'y': 30, 'J': 31, '.': 32, 'H': 33, 'S': 34, 'i': 35, '\\n': 36, 'w': 37, 'U': 38, 'd': 39, 'm': 40, 'v': 41, 'Y': 42, '-': 43, 'L': 44, 'I': 45, ':': 46, 'V': 47, 'R': 48, 'C': 49, 'D': 50, 'M': 51, '2': 52, '1': 53, '0': 54, '5': 55, '[': 56, '#': 57, '4': 58, '9': 59, ']': 60, '8': 61, '*': 62, 'O': 63, 'N': 64, 'K': 65, '/': 66, 'W': 67, 'X': 68, '(': 69, '3': 70, ')': 71, 'Æ': 72, '’': 73, '_': 74, '—': 75, 'æ': 76, '·': 77, 'z': 78, 'x': 79, 'q': 80, ';': 81, '6': 82, '7': 83, 'Q': 84, 'œ': 85, 'μ': 86, 'α': 87, 'λ': 88, 'ο': 89, 'ν': 90, 'ὁ': 91, 'Φ': 92, 'ρ': 93, 'ὑ': 94, 'ξ': 95, '“': 96, '”': 97, '?': 98, '!': 99, '‘': 100, 'é': 101, 'è': 102, '\"': 103, '%': 104, '@': 105, '$': 106}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ3XwNd5ig6k"
      },
      "source": [
        "# Taking the argmax vs. Sampling from a distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8RyylVEipsM"
      },
      "source": [
        "# Let's consider a \"dice\" with five faces a following probability:\n",
        "# 0: 0.2\n",
        "# 1: 0.1\n",
        "# 2: 0.4\n",
        "# 3: 0.2\n",
        "# 4: 0.1\n",
        "\n",
        "dice = torch.tensor([0.2, 0.1, 0.4, 0.2, 0.1], device=DEVICE)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRWJD9LKjfgF",
        "outputId": "a1ff2639-b56e-4970-c558-d16a60d1a11c"
      },
      "source": [
        "# Sampling = roll dice\n",
        "num_rolls = 5\n",
        "\n",
        "for i in range(num_rolls):\n",
        "    print(torch.multinomial(dice, num_samples=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2], device='cuda:0')\n",
            "tensor([4], device='cuda:0')\n",
            "tensor([2], device='cuda:0')\n",
            "tensor([2], device='cuda:0')\n",
            "tensor([2], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJN8dkXZjzb0",
        "outputId": "5f660025-ce7f-436d-b61f-b9886f671a51"
      },
      "source": [
        "# Take the face with the highest probability\n",
        "\n",
        "num_rolls = 5\n",
        "for i in range(num_rolls):\n",
        "    values, indices = torch.topk(dice, k=1, dim=-1)\n",
        "    print(indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2], device='cuda:0')\n",
            "tensor([2], device='cuda:0')\n",
            "tensor([2], device='cuda:0')\n",
            "tensor([2], device='cuda:0')\n",
            "tensor([2], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}